<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <!-- REPLACE: Project Title -->
    <title>EasyMimic: A Low-Cost Framework for Robot Imitation Learning from Human Videos</title>
    <!-- REPLACE: Google Site Verification or remove -->
    <meta name="google-site-verification" content="Your-Verification-Code" />
    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.4.0/css/all.min.css">
    <link href="https://fonts.googleapis.com/css2?family=Noto+Sans:wght@300;400;500;700&display=swap" rel="stylesheet">
    <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
    <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    
    <style>
        body {
            font-family: 'Noto Sans', sans-serif;
            scroll-behavior: smooth;
        }
        .gradient-text {
            background: linear-gradient(to right, #2563eb, #9333ea);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        .hero-bg {
            background: radial-gradient(circle at top right, #eff6ff, #ffffff);
        }
        /* Custom scrollbar for galleries/tables */
        .custom-scroll::-webkit-scrollbar {
            height: 8px;
        }
        .custom-scroll::-webkit-scrollbar-track {
            background: #f1f1f1;
            border-radius: 4px;
        }
        .custom-scroll::-webkit-scrollbar-thumb {
            background: #c7c7c7;
            border-radius: 4px;
        }
        .custom-scroll::-webkit-scrollbar-thumb:hover {
            background: #a8a8a8;
        }
    </style>
</head>
<body class="bg-white text-gray-800">

    <nav class="sticky top-0 z-50 bg-white/90 backdrop-blur-md border-b border-gray-100 shadow-sm">
        <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
            <div class="flex justify-between items-center h-16">
                <div class="flex-shrink-0 font-bold text-xl tracking-tighter cursor-pointer hover:opacity-80 transition" onclick="window.scrollTo(0,0)">
                    <!-- REPLACE: Logo/Name -->
                    <span class="text-blue-600">EasyMimic
                </div>
                <div class="hidden md:flex space-x-8">
                    <!-- REPLACE: Navigation Links -->
                    <a href="#abstract" class="text-gray-600 hover:text-blue-600 transition font-medium">Abstract</a>
                    <a href="#video" class="text-gray-600 hover:text-blue-600 transition font-medium">Video</a>
                    <a href="#method" class="text-gray-600 hover:text-blue-600 transition font-medium">Method</a>
                    <a href="#results" class="text-gray-600 hover:text-blue-600 transition font-medium">Results</a>
                    <a href="#acknowledgments" class="text-gray-600 hover:text-blue-600 transition font-medium">Acknowledgments</a>
                </div>
            </div>
        </div>
    </nav>

    <header class="hero-bg pt-20 pb-16 text-center px-4">
        <div class="max-w-6xl mx-auto">
            <h1 class="text-4xl md:text-6xl font-extrabold mb-8 leading-tight tracking-tight">
                <!-- REPLACE: Hero Title -->
                <span class="gradient-text">EasyMimic</span>: <br class="hidden md:block"> A Low-Cost Robot Imitation Learning Framework from Human Videos
            </h1>
            
            <div class="flex flex-wrap justify-center gap-x-6 gap-y-2 text-lg mb-4 text-blue-600 font-medium">
                <!-- REPLACE: Authors -->
                <a href="#" class="hover:underline hover:text-blue-800 transition relative">Tao Zhang<sup>1*</sup></a>
                <a href="#" class="hover:underline hover:text-blue-800 transition relative">Song Xia<sup>1*</sup></a>
                <a href="#" class="hover:underline hover:text-blue-800 transition relative">Ye Wang<sup>1*‡</sup></a>
                <a href="#" class="hover:underline hover:text-blue-800 transition relative">Qin Jin<sup>1†</sup></a>
            </div>

            <div class="flex flex-col items-center gap-y-2 text-sm md:text-base text-gray-600 mb-10">
                <!-- REPLACE: Affiliations -->
                <div class="flex items-center justify-center">
                    <span class="font-bold mr-1"><sup>1</sup></span> AIM3 Lab, Renmin University of China
                </div>
                <div class="text-xs text-gray-500 mt-2">
                    *Equal contribution, <sup>‡</sup>Project lead, <sup>†</sup>Corresponding author
                </div>
            </div>
            
            <div class="flex justify-center gap-4 mb-12">
                <!-- REPLACE: Links to Paper/Code -->
                <a href="#" class="flex items-center gap-2 bg-gray-900 text-white px-6 py-3 rounded-full hover:bg-gray-700 transition shadow-lg hover:shadow-xl hover:-translate-y-0.5 transform">
                    <i class="fa-regular fa-file-pdf"></i>
                    <span>Paper</span>
                </a>
                <a href="#" class="flex items-center gap-2 bg-gray-800 text-white px-6 py-3 rounded-full hover:bg-gray-600 transition shadow-lg hover:shadow-xl hover:-translate-y-0.5 transform">
                    <i class="fa-brands fa-github"></i>
                    <span>Code</span>
                </a>
            </div>
        </div>
    </header>

    <section id="abstract" class="py-12 bg-white border-b border-gray-100">
        <div class="max-w-4xl mx-auto px-6">
            <h2 class="text-3xl font-bold text-center mb-8 text-gray-900">Abstract</h2>
            <p class="text-justify text-lg leading-relaxed text-gray-600">
                Robot imitation learning is often hindered by the high cost of collecting large-scale, real-world data. 
                This challenge is especially significant for low-cost robots designed for home use, as they must be both user-friendly and affordable.
                To address this, we propose the <strong>EasyMimic framework</strong>, 
                a low-cost and replicable solution that enables robots to quickly learn manipulation policies from human video demonstrations captured with standard RGB cameras.
                Our method first extracts 3D hand trajectories from the videos. 
                An action alignment module then maps these trajectories to the gripper control space of a low-cost robot. 
                To bridge the human-to-robot domain gap, we introduce a simple and user-friendly hand visual augmentation strategy. 
                We then use a co-training method, fine-tuning a model on both the processed human data and a small amount of robot data, enabling rapid adaptation to new tasks.
                Experiments on the low-cost LeRobot platform demonstrate that EasyMimic achieves high performance across various manipulation tasks. 
                It significantly reduces the reliance on expensive robot data collection, offering a practical path for bringing intelligent robots into homes.
            </p>
        </div>
    </section>

    <!-- New Section: Low-cost Framework Overview -->
    <section id="overview" class="py-16 bg-gray-50">
        <div class="max-w-6xl mx-auto px-6">
            <h2 class="text-3xl font-bold text-center mb-10 text-gray-900">
                Low-Cost Framework Overview
            </h2>
            <div class="bg-white rounded-2xl overflow-hidden shadow-xl border border-gray-200">
                <!-- Using fig/Lowcostframework.png -->
                <img src="assets/fig/Lowcostframework.png" alt="Low Cost Framework" class="w-full h-auto object-contain p-4">
                <div class="p-6 border-t border-gray-100 bg-gray-50">
                    <p class="text-gray-700 text-center">
                        <strong>Figure 1:</strong> Overview of our proposed low-cost framework. It efficiently enables robot imitation learning from human videos with minimal computational resources.
                    </p>
                </div>
            </div>
        </div>
    </section>

    <section id="video" class="py-16 bg-gray-50">
        <div class="max-w-5xl mx-auto px-6">
            <h2 class="text-3xl font-bold text-center mb-10 text-gray-900">
                <i class="fa-solid fa-clapperboard text-blue-600 mr-2"></i> Demo Video
            </h2>
            <div class="rounded-2xl overflow-hidden shadow-2xl border border-gray-200 bg-black">
                <video class="w-full aspect-video" controls>
                    <source src="assets/videos/a4b5ea2b2421f66b302fbf1d5bb93895.mp4" type="video/mp4">
                </video>
            </div>
            <p class="text-center mt-4 text-gray-600">
                Demo video showcasing EasyMimic performing various manipulation tasks.
            </p>
        </div>
    </section>

    <section id="method" class="py-20 bg-white">
        <div class="max-w-6xl mx-auto px-6">
            <h2 class="text-3xl font-bold text-center mb-16 text-gray-900">Methodology</h2>
            
            <!-- Subsection 1: Physical Alignment -->
            <div class="mb-20">
                <div class="flex items-center gap-4 mb-8 border-b border-gray-100 pb-4">
                    <span class="bg-blue-100 text-blue-800 text-lg font-bold px-4 py-1.5 rounded-full">Part 1</span>
                    <h3 class="text-2xl font-bold text-gray-800">Physical Alignment</h3>
                </div>
                <p class="text-gray-600 leading-relaxed text-lg mb-6">
                    Our method first extracts 3D hand trajectories from the RGB videos. The physical alignment module then maps these trajectories to the gripper control space of the robot, ensuring precise motion transfer from human demonstrations to robot execution.
                </p>
                
                <div class="bg-white p-2 md:p-4 rounded-2xl shadow-lg border border-gray-100 mb-6">
                    <div class="rounded-xl overflow-hidden bg-white">
                        <img src="assets/fig/retargeting_framework.png" alt="Physical Alignment" class="w-full h-auto object-contain">
                    </div>
                    <p class="mt-4 text-gray-600 text-center text-sm font-medium">
                        <strong>Figure 2: Physical Alignment.</strong> 
                        The physical alignment module maps extracted human hand trajectories to the robot's gripper control space.
                    </p>
                </div>
            </div>

            <!-- Subsection 2: Co-Training Strategy -->
            <div>
                <div class="flex items-center gap-4 mb-8 border-b border-gray-100 pb-4">
                    <span class="bg-purple-100 text-purple-800 text-lg font-bold px-4 py-1.5 rounded-full">Part 2</span>
                    <h3 class="text-2xl font-bold text-gray-800">Co-Training Strategy</h3>
                </div>
                
                <div class="bg-white p-2 md:p-4 rounded-2xl shadow-lg border border-gray-100 mb-6">
                    <div class="rounded-xl overflow-hidden bg-white">
                        <img src="assets/fig/trainingstrategy.png" alt="Co-Training Strategy" class="w-full h-auto object-contain">
                    </div>
                    <p class="mt-4 text-gray-600 text-center text-sm font-medium">
                        <strong>Figure 3: Co-Training Strategy.</strong> 
                        Overview of the co-training process leveraging both human and robot data for robust policy learning.
                    </p>
                </div>
                <p class="text-gray-600 leading-relaxed text-lg">
                    We employ a co-training strategy that fine-tunes the policy on both the processed human data and a small amount of robot data. This enables the robot to rapidly adapt to new tasks and generalize effectively across different manipulation scenarios.
                </p>
            </div>

        </div>
    </section>

    <section id="results" class="py-20 bg-gray-50">
        <div class="max-w-7xl mx-auto px-6">
            <h2 class="text-3xl font-bold text-center mb-4 text-gray-900">Experimental Results</h2>
            <div class="max-w-4xl mx-auto text-lg text-gray-700 mb-12 leading-relaxed">
                In this section, we introduce the manipulation tasks and experimental setups. 
                We then present a comparison with baseline methods and provide a detailed analysis to validate the effectiveness of our approach.
            </div>

            <!-- Experimental Setup -->
            <div class="mb-16">
                <h3 class="text-2xl font-bold text-gray-800 mb-6 border-l-4 border-blue-600 pl-4">1. Experimental Setup</h3>
                
                <div class="bg-white rounded-xl shadow-lg border border-gray-100 p-8 mb-8">
                    <h4 class="text-xl font-bold text-gray-800 mb-4">Hardware and Tasks</h4>
                    <p class="text-gray-600 mb-4 leading-relaxed">
                        Our experimental platform uses a 6-DoF so100-plus robotic arm equipped with a two-finger gripper.
                        The vision system includes two monocular RGB cameras: one fixed above the robot's base for a top-down global view, and another mounted on the wrist for an end-effector-centric first-person view.
                        We evaluate our method on four tabletop manipulation tasks:
                    </p>
                    <ul class="grid md:grid-cols-2 gap-4">
                        <li class="bg-gray-50 p-4 rounded-lg border border-gray-100">
                            <span class="font-bold text-blue-700">Pick and Place (Pick):</span>
                            <p class="text-sm text-gray-600 mt-2">The robot picks up a toy duck and places it into a bowl. Success (1.0) = grasping (0.5) + placing (0.5).</p>
                        </li>
                        <li class="bg-gray-50 p-4 rounded-lg border border-gray-100">
                            <span class="font-bold text-blue-700">Pull and Push (Pull):</span>
                            <p class="text-sm text-gray-600 mt-2">Robot pulls open a drawer and pushes it closed. Success (1.0) = pulling (0.5) + pushing (0.5).</p>
                        </li>
                        <li class="bg-gray-50 p-4 rounded-lg border border-gray-100">
                            <span class="font-bold text-blue-700">Stacking (Stack):</span>
                            <p class="text-sm text-gray-600 mt-2">Stack a cube on a block, then a pyramid on the cube. Each stack awards 0.5 points.</p>
                        </li>
                        <li class="bg-gray-50 p-4 rounded-lg border border-gray-100">
                            <span class="font-bold text-blue-700">Language Conditioned (LC):</span>
                            <p class="text-sm text-gray-600 mt-2">Execute natural language instructions specifying target object and goal (e.g., "pick up the pink duck...").</p>
                        </li>
                    </ul>
                </div>

                <div class="grid md:grid-cols-2 gap-8">
                    <div class="bg-white rounded-xl shadow-lg border border-gray-100 p-8">
                        <h4 class="text-xl font-bold text-gray-800 mb-4">Model & Training</h4>
                        <p class="text-gray-600 text-sm leading-relaxed mb-4">
                            We use the pre-trained Gr00T N1.5-3B foundation VLA model. 
                            The policy network outputs absolute actions (6-DoF end-effector poses + gripper states).
                            Training: 5,000 gradient steps, AdamW optimizer (lr=1e-4), batch size 32, on a single NVIDIA RTX 4090 GPU.
                        </p>
                    </div>
                    <div class="bg-white rounded-xl shadow-lg border border-gray-100 p-8">
                        <h4 class="text-xl font-bold text-gray-800 mb-4">Data Collection</h4>
                        <p class="text-gray-600 text-sm leading-relaxed mb-4">
                            For each task: 100 human video demonstrations vs. 20 robot teleoperation trajectories.
                            Human data collection rate is substantially higher (up to 12.5 demos/min) compared to robot teleoperation (max 2 demos/min), highlighting efficiency.
                        </p>
                    </div>
                </div>
            </div>

            <!-- Main Results -->
            <div class="mb-16">
                <h3 class="text-2xl font-bold text-gray-800 mb-6 border-l-4 border-indigo-600 pl-4">2. Main Results</h3>
                
                <div class="bg-white rounded-xl shadow-lg border border-gray-100 p-8 mb-8 overflow-x-auto">
                    <h4 class="text-xl font-bold text-gray-800 mb-6">Comparison of Training Strategies</h4>
                    <table class="w-full text-left border-collapse">
                        <thead>
                            <tr class="border-b-2 border-gray-200">
                                <th class="py-3 px-4 text-gray-700 font-bold">Strategy</th>
                                <th class="py-3 px-4 text-gray-700 font-bold">Pick</th>
                                <th class="py-3 px-4 text-gray-700 font-bold">Pull</th>
                                <th class="py-3 px-4 text-gray-700 font-bold">Stack</th>
                                <th class="py-3 px-4 text-gray-700 font-bold">LC</th>
                                <th class="py-3 px-4 text-blue-700 font-bold">Avg.</th>
                            </tr>
                        </thead>
                        <tbody>
                            <tr class="border-b border-gray-100 hover:bg-gray-50">
                                <td class="py-3 px-4 text-gray-600">Robot-Only (10 traj.)</td>
                                <td class="py-3 px-4 text-gray-600">0.30</td>
                                <td class="py-3 px-4 text-gray-600">0.30</td>
                                <td class="py-3 px-4 text-gray-600">0.15</td>
                                <td class="py-3 px-4 text-gray-600">0.30</td>
                                <td class="py-3 px-4 text-gray-800 font-medium">0.26</td>
                            </tr>
                            <tr class="border-b border-gray-100 hover:bg-gray-50">
                                <td class="py-3 px-4 text-gray-600">Robot-Only (20 traj.)</td>
                                <td class="py-3 px-4 text-gray-600">0.60</td>
                                <td class="py-3 px-4 text-gray-600">0.70</td>
                                <td class="py-3 px-4 text-gray-600">0.35</td>
                                <td class="py-3 px-4 text-gray-600">0.40</td>
                                <td class="py-3 px-4 text-gray-800 font-medium">0.51</td>
                            </tr>
                            <tr class="border-b border-gray-100 hover:bg-gray-50">
                                <td class="py-3 px-4 text-gray-600">Pretrain-Finetune</td>
                                <td class="py-3 px-4 text-gray-600">0.80</td>
                                <td class="py-3 px-4 text-gray-800 font-bold">0.90</td>
                                <td class="py-3 px-4 text-gray-600">0.50</td>
                                <td class="py-3 px-4 text-gray-600">0.80</td>
                                <td class="py-3 px-4 text-gray-800 font-medium">0.75</td>
                            </tr>
                            <tr class="bg-blue-50 border-b border-blue-100">
                                <td class="py-3 px-4 text-blue-800 font-bold">EasyMimic (Ours)</td>
                                <td class="py-3 px-4 text-blue-800 font-bold">1.00</td>
                                <td class="py-3 px-4 text-blue-800 font-bold">0.90</td>
                                <td class="py-3 px-4 text-blue-800 font-bold">0.70</td>
                                <td class="py-3 px-4 text-blue-800 font-bold">0.90</td>
                                <td class="py-3 px-4 text-blue-800 font-bold">0.88</td>
                            </tr>
                        </tbody>
                    </table>
                    <p class="mt-4 text-gray-600 text-sm">
                        Table 2: Performance evaluation. EasyMimic achieves the best performance (0.88 avg), surpassing baselines significantly while maintaining data efficiency.
                    </p>
                </div>
                
                <p class="text-gray-600 leading-relaxed mb-6">
                    <strong>Comparison Analysis:</strong> Training with scarce robot data (10-20 trajectories) yields limited performance (0.26-0.51). 
                    Incorporating human data boosts performance significantly. EasyMimic outperforms Pretrain-Finetune by 0.13 points and Robot-Only (10 traj) by 0.62 points, showing that co-training effectively leverages both human and robot data strengths.
                </p>

                <div class="bg-gray-50 p-6 rounded-xl border-l-4 border-green-500">
                    <h4 class="text-lg font-bold text-gray-800 mb-2">Language Condition Performance</h4>
                    <p class="text-gray-600 text-sm">
                        In language-conditioned tasks involving distinct object-goal combinations (e.g., pink/green duck into bowl/block), EasyMimic achieves a success rate of <strong>0.90</strong>, compared to just 0.40 for the robot-only baseline. This demonstrates substantial value in utilizing human demonstrations for complex instruction following.
                    </p>
                </div>
            </div>

            <!-- Further Analysis -->
            <div class="mb-16">
                <h3 class="text-2xl font-bold text-gray-800 mb-6 border-l-4 border-purple-600 pl-4">3. Further Analysis</h3>
                
                <!-- Dataset Size -->
                <div class="mb-12">
                     <h4 class="text-xl font-bold text-gray-800 mb-4">Effect of Data Scale</h4>
                     <div class="grid md:grid-cols-2 gap-8 mb-4">
                        <div class="bg-white p-2 rounded-xl shadow-lg border border-gray-200">
                            <img src="assets/fig/human_number.png" alt="Effect of Human Data" class="w-full h-auto object-contain">
                            <p class="mt-2 text-center text-sm text-gray-600 font-medium">Figure 4(a): Varying human data (fixed 10 robot traj).</p>
                        </div>
                        <div class="bg-white p-2 rounded-xl shadow-lg border border-gray-200">
                            <img src="assets/fig/robot_number.png" alt="Effect of Robot Data" class="w-full h-auto object-contain">
                            <p class="mt-2 text-center text-sm text-gray-600 font-medium">Figure 4(b): Varying robot data (fixed 50 human videos).</p>
                        </div>
                    </div>
                    <p class="text-gray-600 leading-relaxed">
                        Increasing human demonstrations consistently improves performance, with optimal gains up to ~50 demos. 
                        For robot data, performance saturates quickly after 10-20 trajectories when complemented by human data. 
                        This confirms that high performance can be achieved with minimal expensive robot data if sufficient human data is available.
                    </p>
                </div>

                <!-- Alignment & Action Heads -->
                <div class="grid md:grid-cols-2 gap-8 mb-12">
                     <div class="bg-white p-6 rounded-xl shadow border border-gray-100">
                        <h4 class="text-lg font-bold text-gray-800 mb-4">Ablation: Alignment Strategies</h4>
                        <ul class="space-y-3 text-sm text-gray-600">
                            <li class="flex justify-between border-b pb-2">
                                <span><strong>EasyMimic (Full)</strong></span>
                                <span class="font-bold text-blue-600">0.87</span>
                            </li>
                            <li class="flex justify-between border-b pb-2">
                                <span>w/o Action Alignment (AA)</span>
                                <span>0.60 (-0.27)</span>
                            </li>
                            <li class="flex justify-between border-b pb-2">
                                <span>w/o Visual Alignment (VA)</span>
                                <span>0.40 (-0.47)</span>
                            </li>
                            <li class="flex justify-between">
                                <span>VA-Partial Masking</span>
                                <span>0.27 (-0.60)</span>
                            </li>
                        </ul>
                        <p class="mt-4 text-xs text-gray-500">
                            Both Action Alignment (AA) and Visual Alignment (VA) are critical. VA-Partial (masking only fingers) performs poorly, indicating full-hand augmentation is necessary.
                        </p>
                     </div>

                     <div class="bg-white p-6 rounded-xl shadow border border-gray-100">
                        <h4 class="text-lg font-bold text-gray-800 mb-4">Ablation: Pretraining & Heads</h4>
                        <ul class="space-y-3 text-sm text-gray-600">
                            <li class="flex justify-between border-b pb-2">
                                <span><strong>EasyMimic (Indep. Heads)</strong></span>
                                <span class="font-bold text-blue-600">0.87</span>
                            </li>
                            <li class="flex justify-between border-b pb-2">
                                <span>Shared Action Head</span>
                                <span>0.47 (-0.40)</span>
                            </li>
                            <li class="flex justify-between border-b pb-2">
                                <span>w/o Pretraining (EasyMimic)</span>
                                <span>0.53</span>
                            </li>
                             <li class="flex justify-between">
                                <span>w/o Pretraining (Robot-Only)</span>
                                <span>0.15</span>
                            </li>
                        </ul>
                        <p class="mt-4 text-xs text-gray-500">
                            Independent action heads prevent interference between human/robot data. Combining EasyMimic with pre-training yields the best results.
                        </p>
                     </div>
                </div>

                <!-- Case Study -->
                <div class="mb-12">
                    <h4 class="text-xl font-bold text-gray-800 mb-4">Case Analysis & Visual Alignment</h4>
                     <div class="grid md:grid-cols-2 gap-8">
                        <div class="bg-white p-2 rounded-xl shadow-lg border border-gray-200">
                            <div class="overflow-hidden rounded-lg bg-white">
                                <img src="assets/fig/case_study.png" alt="Failure Case Study" class="w-full h-auto object-contain">
                            </div>
                            <p class="mt-4 text-sm text-gray-600 px-2 pb-2 text-center">
                                <strong>Figure 5: Failure Modes.</strong> (a) Premature release, (b) Imprecise grasp, (c) Collision, (d) Unstable placement.
                            </p>
                        </div>

                         <div class="bg-white p-2 rounded-xl shadow-lg border border-gray-200">
                            <div class="overflow-hidden rounded-lg bg-white">
                                <img src="assets/fig/VA_case.png" alt="Visual Alignment Module" class="w-full h-auto object-contain">
                            </div>
                            <p class="mt-4 text-sm text-gray-600 px-2 pb-2 text-center">
                                <strong>Figure 6: Visual Alignment.</strong> Comparison of original human hand, partial masking, and full masking (VA-Full).
                            </p>
                        </div>
                    </div>
                </div>

                <!-- Generalization -->
                <div class="bg-yellow-50 rounded-xl border border-yellow-200 p-6">
                    <h4 class="text-lg font-bold text-gray-800 mb-3">Zero-Shot Generalization</h4>
                    <p class="text-gray-700 text-sm mb-4">
                        Tested on unseen objects (Green Duck, Pink Cube) after training only on Pink Duck.
                    </p>
                    <div class="grid grid-cols-3 gap-4 text-center">
                        <div class="bg-white p-3 rounded shadow-sm">
                            <div class="text-xs text-gray-500 uppercase font-bold">Unseen Object</div>
                            <div class="font-bold text-gray-800">Avg. Score</div>
                        </div>
                        <div class="bg-white p-3 rounded shadow-sm opacity-60">
                            <div class="text-xs text-gray-500 uppercase">Robot-Only</div>
                            <div class="font-bold text-gray-400">0.35</div>
                        </div>
                        <div class="bg-white p-3 rounded shadow-sm border-2 border-yellow-400">
                            <div class="text-xs text-gray-500 uppercase text-yellow-700">EasyMimic</div>
                            <div class="font-bold text-yellow-700">0.65</div>
                        </div>
                    </div>
                </div>

            </div>
            
        </div>
    </section>

    <!-- Removed Acknowledgments as requested -->

    <section id="bibtex" class="py-16 bg-white">

    <section id="bibtex" class="py-16 bg-white">
        <div class="max-w-4xl mx-auto px-6">
            <h2 class="text-3xl font-bold text-gray-900 mb-6">Citation</h2>
            <div class="relative bg-gray-900 rounded-xl p-6 shadow-xl overflow-hidden group">
                <button onclick="copyBibtex()" class="absolute top-4 right-4 text-gray-400 hover:text-white transition p-2 rounded-md hover:bg-gray-700" title="Copy to Clipboard">
                    <i class="fa-regular fa-copy"></i>
                </button>
                <pre class="text-gray-300 font-mono text-sm overflow-x-auto whitespace-pre-wrap leading-relaxed" id="bibtex-content">
@misc{your_project_citation,
      title={[Your Project Title]}, 
      author={[Author Names]},
      year={[Year]},
      eprint={[ID]},
      archivePrefix={arXiv},
      primaryClass={cs.CV},
      url={[URL]}, 
}
</pre>
            </div>
        </div>
    </section>

    <footer class="bg-white border-t border-gray-100 py-10 text-center text-gray-500 text-sm">
        <p>© 2026 [Your Project Name]. All Rights Reserved.</p>
        <div class="mt-4 flex justify-center space-x-4">
            <a href="#" class="hover:text-blue-600 transition"><i class="fa-brands fa-github text-xl"></i></a>
            <a href="#" class="hover:text-blue-600 transition"><i class="fa-brands fa-twitter text-xl"></i></a>
        </div>
    </footer>

    <script>
        function copyBibtex() {
            const content = document.getElementById('bibtex-content').innerText;
            navigator.clipboard.writeText(content).then(() => {
                const btn = document.querySelector('button[onclick="copyBibtex()"] i');
                btn.className = 'fa-solid fa-check text-green-400';
                setTimeout(() => {
                    btn.className = 'fa-regular fa-copy';
                }, 2000);
            }).catch(err => {
                console.error('Failed to copy: ', err);
            });
        }
    </script>
</body>
</html>